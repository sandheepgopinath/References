---
attachments: [Clipboard_2022-03-11-21-10-03.png, Clipboard_2022-03-11-21-29-24.png, Clipboard_2022-03-11-21-42-54.png, Clipboard_2022-03-12-11-00-26.png, Clipboard_2022-03-12-11-00-43.png]
tags: [Notebooks/NLP]
title: 1.Dense Encoding
created: '2022-03-11T15:31:27.867Z'
modified: '2022-03-12T10:44:22.717Z'
---

# 1.Dense Encoding

- One Hot encoding vectors are sparse. ie has zero values 
- To encode them into a lower dimension of more dense values we use One Hot encoding
- In Sparse veectors, the words nearby do not have any relation.
- After dense encoding, the words in neightborhood have some relation
- Easy to add new words into the same D Dimensional space

## How to do it?

- Co-Occurence matrix
1. I enjoy flying.
2. I like NLP.
3. I like deep learning

![](@attachment/Clipboard_2022-03-12-11-00-26.png)
- The Co-Occurence matrix can get big as the vocabulary gets bigger. 
- Hence we use SVD to make it smaller
- SVD However is computationaly expensive
![](@attachment/Clipboard_2022-03-12-11-00-43.png)




Reference : https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf

