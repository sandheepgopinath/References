		#### Multi Layer LSTM
![[Pasted image 20220402220539.png]]
- The hidden layers can have a different structure from the first layer. This is analogous to DNN
- Variations
	- Bi-directional LSTM
	 ![[Pasted image 20220402220651.png]]
	- The forward layer works exactly in the same way as above
	- The backward layer feeds the output back into the previous later. 
	- The input is fed to both the forward layer and the backward layer
	- The backward layer has data coming from the input as well as the next layer. 
	- This helps in applications like POS tagging. 
	 
#### Attention based mechanism
![[Pasted image 20220402221223.png]]
![[Pasted image 20220402221312.png]]  Here we can see that the word "street" gets more attention based on its context.  (Winograd schema)

- **Attention Architecture**
![[Pasted image 20220402221625.png]]
![[Pasted image 20220403011806.png]]

#### General Attention architecture
![[Pasted image 20220402234959.png]]
In the above architecture, the entire sentence is considered which can get computationally expensive. To make it lighter and faster we use local weights which uses a smaller context window. 
![[Pasted image 20220402234942.png]]


#### BERT

![[Pasted image 20220403095810.png]]

- Pretrained weights for BERT can be obtained from Hugging face
- Based on the requirement, we can further do finetuning on BERT
- 