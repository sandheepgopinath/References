---
attachments: [Clipboard_2022-03-13-10-35-22.png, Clipboard_2022-03-13-10-35-31.png, Clipboard_2022-03-13-10-46-53.png, Clipboard_2022-03-13-10-47-24.png, Clipboard_2022-03-13-10-47-47.png, Clipboard_2022-03-13-10-48-34.png]
tags: [Notebooks/NLP]
title: 3.Glove Vectors
created: '2022-03-12T07:33:59.888Z'
modified: '2022-03-13T05:18:36.784Z'
---

# 3.Glove Vectors

- Global vectors for word embedding
- The Skipgram of CBOW uses only nearest neighbors to get context of words
- Glove starts with Co-occurence matrix 
- Notations
  - $X_i = {\Sigma}_k X_ik$ : Number of times any word appeard in conext of word i


# Process

- From the co-occurrence matrix the conditional probability is taken for all the words
- he probabilities are used to represent each words against a set of reference words of a fixed dimension
- These embeddings are then used for creating vector representations
- These vectors, since it is created from conditional probabilities has meanings and context embedded into them
- Arithematic operations can be done on them to find similarity ( Details in case study ) 
![](@attachment/Clipboard_2022-03-13-10-35-31.png)
- A sliding window will be used to calculate the probability of occurence
![](@attachment/Clipboard_2022-03-13-10-46-53.png)

- Words will be weighted based on lexicographic distance
![](@attachment/Clipboard_2022-03-13-10-47-24.png)

- Log of counts are taken to reduce the blowup of count for words like should and have which always occur together
![](@attachment/Clipboard_2022-03-13-10-47-47.png)


![](@attachment/Clipboard_2022-03-13-10-48-34.png)


Reading : https://nlp.stanford.edu/pubs/glove.pdf
Video References : https://www.youtube.com/watch?v=Fn_U2OG1uqI

