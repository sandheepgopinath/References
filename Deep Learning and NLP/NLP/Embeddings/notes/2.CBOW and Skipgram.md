---
attachments: [Clipboard_2022-03-11-21-45-27.png, Clipboard_2022-03-11-21-47-19.png, Clipboard_2022-03-11-21-47-31.png, Clipboard_2022-03-11-21-54-42.png, Clipboard_2022-03-12-11-01-07.png, Clipboard_2022-03-12-11-01-18.png]
tags: [Notebooks/NLP]
title: 2.CBOW and Skipgram
created: '2022-03-11T16:13:08.398Z'
modified: '2022-03-12T10:44:40.990Z'
---

# 2.CBOW and Skipgram


![](@attachment/Clipboard_2022-03-12-11-01-07.png)
 
- It is then used to generate word embeddings
In this case, multipple inputs are given to the model which will then predict the word based on the context. 
- Once the model when is able to predict the missing words properly, it can be used to predict missing words
- We can also remove the final softmax layer and use it for generating word embeddings. 
- These are usually trained on large corpus and hence is better than creating the word embeddings from scratch
- If we are working in a new/ specific domain like legal/ medical, then creating a new CBOW manually might be useful. It would still be better to reuse whatever is available before creating new models



![](@attachment/Clipboard_2022-03-12-11-01-18.png)
- A skip gram is the opposite of a CBOW. In this case, 
- In Skipgram we are tying to predict the next or previous word based on a current word. 
- Datasets fed into the model masking the surrounding words and the model learns to predict the next or previous words based on the contexts in which these words has occured eralier. 
- This is how we train the model


#### Word analogies
- The vectors created from CBOW can be used to find analogs
#### Points to keep in mind
- Larger dimensions are more expressive
- Smaller dimensions trains faster
- Gain will saturate at one point and will not increase even if we inrease the vector dimension

Reference : https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf

